{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0548e959-2f22-42d0-86ec-f2dc190fb01c",
   "metadata": {},
   "source": [
    "# _LeNet-5 Convolutional Neural Network with MNIST_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697c8e53-b69b-46ac-a1be-e200239a91d0",
   "metadata": {},
   "source": [
    "<img src='cnn.webp'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2203bdc-fe62-4c8f-b89f-dd964bc6b320",
   "metadata": {},
   "source": [
    "_Bu proje, LeNet-5 mimarisini kullanarak **el yazısı rakamları tanıma** üzerine bir Convolutional Neural Network (CNN) uygulamasıdır. LeNet-5, 1998 yılında Yann LeCun tarafından geliştirilmiş ve görüntü sınıflandırma görevleri için öncü bir mimaridir. Bu proje kapsamında LeNet-5’in orijinal katman yapısı korunmuş ve Python Keras kütüphanesi kullanılarak uygulanmıştır._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a585ce03-6757-4157-91a0-052e86ce8eea",
   "metadata": {},
   "source": [
    "**_Neden MNIST Dataset Kullanıldı?_** <br>\n",
    "_Bu proje için veri seti belirtilmemiştir. LeNet-5 mimarisi **orijinal olarak el yazısı rakamları tanımak için geliştirilmiştir**, bu nedenle **MNIST veri seti** seçilmiştir. MNIST, 0-9 arası el yazısı rakamlardan oluşan standart bir benchmark veri setidir ve CNN’lerin performansını değerlendirmek için yaygın olarak kullanılır._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfa45e9-5d5a-4f83-b8c7-264b0371d311",
   "metadata": {},
   "source": [
    "**_Veri Setinin İçeriği_**\n",
    "- _Toplam 70.000 görüntü: 60.000 eğitim ve 10.000 test görüntüsü_\n",
    "- _Her görüntü: 28×28 boyutunda gri tonlamalı (grayscale)_\n",
    "- _10 sınıf: Rakamlar 0’dan 9’a kadar_\n",
    "- _Piksel değerleri: 0–255 arasında (kodda normalize edilerek 0–1 aralığına getirilmiştir)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f55f55-be2a-45ed-9d5a-3a1a24faf197",
   "metadata": {},
   "source": [
    "## _Import_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f1364d2-eb26-4f84-9da2-ab83c16d0832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedde32-7c19-4004-bc37-2fddedbcaab2",
   "metadata": {},
   "source": [
    "## _Load MNIST dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cf888ce-7c27-4666-b856-41697928bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabcab2b-99cb-4eed-9736-1eeca919ced6",
   "metadata": {},
   "source": [
    "## _Preprocess data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "574abe43-c67f-4674-8c3b-5b29de63b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize to 32x32 (original LeNet-5 input)\n",
    "X_train = tf.image.resize(tf.expand_dims(X_train, -1), [32, 32])\n",
    "X_test = tf.image.resize(tf.expand_dims(X_test, -1), [32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95fffb97-f8d5-4c1e-8be6-1be532e0426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3fd8bc4-8b1b-465f-b31c-552dd6bc0562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de32615-f14f-4c69-a7ef-28bee64ff998",
   "metadata": {},
   "source": [
    "## _Build LeNet-5 model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f148eb2-e0dd-494b-afe5-467a8633319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(32,32,1)))\n",
    "\n",
    "# C1 Convolutional Layer\n",
    "model.add(Conv2D(filters=6, kernel_size=(5,5), activation='tanh', padding='same'))\n",
    "\n",
    "# S2 Pooling Layer (Average Pooling)\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "# C3 Convolutional Layer\n",
    "model.add(Conv2D(filters=16, kernel_size=(5,5), activation='tanh'))\n",
    "\n",
    "# S4 Pooling Layer (Average Pooling)\n",
    "model.add(AveragePooling2D(pool_size=(2,2), strides=2))\n",
    "\n",
    "# C5 Fully Connected Convolutional Layer\n",
    "model.add(Conv2D(filters=120, kernel_size=(5,5), activation='tanh'))\n",
    "\n",
    "# Flatten before F6\n",
    "model.add(Flatten())\n",
    "\n",
    "# F6 Fully Connected Layer\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681d7fb-58e6-414f-9a2d-54ddf4cb8e5d",
   "metadata": {},
   "source": [
    "## _Compile the model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10118b69-22b2-45d5-a365-e8998a4e8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc568e2a-9437-417a-8797-7bb6933d51b8",
   "metadata": {},
   "source": [
    "## _Train the model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64105b52-b13c-4a63-86d3-b434ce2fc319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9068 - loss: 0.3196 - val_accuracy: 0.9630 - val_loss: 0.1371\n",
      "Epoch 2/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9605 - loss: 0.1303 - val_accuracy: 0.9773 - val_loss: 0.0850\n",
      "Epoch 3/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9729 - loss: 0.0876 - val_accuracy: 0.9815 - val_loss: 0.0710\n",
      "Epoch 4/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9808 - loss: 0.0635 - val_accuracy: 0.9817 - val_loss: 0.0689\n",
      "Epoch 5/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9850 - loss: 0.0501 - val_accuracy: 0.9847 - val_loss: 0.0548\n",
      "Epoch 6/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0393 - val_accuracy: 0.9833 - val_loss: 0.0583\n",
      "Epoch 7/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9904 - loss: 0.0315 - val_accuracy: 0.9837 - val_loss: 0.0549\n",
      "Epoch 8/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9917 - loss: 0.0264 - val_accuracy: 0.9832 - val_loss: 0.0597\n",
      "Epoch 9/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9930 - loss: 0.0227 - val_accuracy: 0.9848 - val_loss: 0.0537\n",
      "Epoch 10/10\n",
      "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.9951 - loss: 0.0170 - val_accuracy: 0.9875 - val_loss: 0.0512\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_split=0.1, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e45d638-f7be-4a09-b103-c91e61bb5298",
   "metadata": {},
   "source": [
    "## _Evaluate the model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a74e8d6a-7050-4f54-86d7-e02a81fe1563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0458 \n",
      "Test Accuracy: 98.61%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc750434-5721-4d0d-8cf6-ee902241a1ee",
   "metadata": {},
   "source": [
    "## _Save Model_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b824d799-eef1-4ca1-bd8d-82fba635f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lenet5_mnist.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb35a01-c000-49b5-abca-c7bd24f0bf08",
   "metadata": {},
   "source": [
    "**_Model Performansı_**\n",
    "- _Eğitim sırasında validation accuracy gözlemlenmiştir_\n",
    "- _Test seti üzerinde model doğruluk oranı yaklaşık **%98** civarındadır._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
